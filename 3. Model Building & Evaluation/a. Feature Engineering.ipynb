{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecead2c4",
   "metadata": {},
   "source": [
    "# Importing our dataset (Final_Dataset.pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89594584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "merged_df = pd.read_pickle(\"Final_Dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17ad7f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2065 entries, 0 to 2089\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    2065 non-null   object\n",
      " 1   label   2065 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 48.4+ KB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2301bbf",
   "metadata": {},
   "source": [
    "# Feature Engineering Plan\n",
    "\n",
    "This plan outlines the key features to be engineered for distinguishing between human and AI-generated texts, focusing on linguistic, readability, n-gram, emotional and semantic coherence. These features aim to capture the differentiation between AI from human writing styles and content.\n",
    "\n",
    "### 1. Linguistic Features\n",
    "These features aim to capture the essence of how texts are constructed, focusing on the structural and lexical aspects.\n",
    "\n",
    "- **Syntactic Complexity**: Syntactic complexity looks at how complex the structure of sentences is. Since AI might not always mimic the wide range of ways humans put sentences together, this aspect is key in figuring out who wrote something, highlighting the skillful way words are woven together (Lu, 2010).\n",
    "- **Lexical Richness**: Lexical richness is about the variety of words used. AI-written texts might use words differently because of what they've learned from their data, so looking at the Type-Token Ratio (TTR) and other similar measures can point out differences in how complex and unique the text is (McCarthy and Jarvis, 2010).\n",
    "- **Burstiness**: Burstiness tells us about how often different words are used in a text, shedding light on whether the language feels natural or machine-made. This can help spot the unique patterns AI uses in creating text (Church and Gale, 1995).\n",
    "- **Perplexity**: Perplexity measures how predictable a text is, with AI-written texts often being more predictable because of how their algorithms tend to pick more expected words (Juola, 2006). This is checked here, using a language model - GPT-2.\n",
    "- **Semantic Coherence**: Semantic coherence is all about how logically and consistently ideas are connected. Human writing usually flows better and links ideas more smoothly than AI, which can sometimes jump from one idea to another less gracefully..\n",
    "\n",
    "### 2. Readability Feature\n",
    "Readability scores help figure out how easy or hard a text is to understand. Since AI might not really focus on making texts match certain levels of difficulty the way people do, checking these scores can point out texts where the difficulty level hasn't been adjusted on purpose by a human writer (Kincaid et al., 1975).\n",
    "\n",
    "### 3. N-Gram Feature\n",
    "Looking at n-grams, which are groups of words that appear together, helps us see patterns in language that are important for figuring out who wrote something. AI-written texts might keep using the same n-grams over and over because those are the ones they learned from their data, showing a different pattern than texts written by people (Stamatatos, 2009). Hence, checking unique N-grams here. \n",
    "\n",
    "### 4. Sentiment Feature\n",
    "Sentiment analysis checks the emotional tone of texts. People can write with a rich mix of emotions, but AI might either not get the subtlety right or might show emotions in a more predictable way. This difference can help us tell apart texts written by humans from those generated by AI (Liu, 2012).\n",
    "\n",
    "Using these specialized methods, we're working on building a solid system that can tell the difference between the intricate details of human-written texts and those created by AI, helping us better understand how artificial content stacks up against human creativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05a78e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import textstat\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import BertModel, BertTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "import nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62083fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d12ba2a84b045ea9d911400b480d370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\PC\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17c1f322ebb4537b2e5e61dede80fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eda38a337de4ef0886491c9282dc774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73adac99bd7a4d329ee7fe629fcd5949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Converting labels to binary\n",
    "merged_df['label'] = (merged_df['label'] == 'AI-written').astype(int)\n",
    "\n",
    "# Function to calculate Flesch Reading Ease score using textstat\n",
    "def readability_score(text):\n",
    "    return textstat.flesch_reading_ease(text)\n",
    "\n",
    "# Applying the readability score function to our data\n",
    "merged_df['readability_score'] = merged_df['text'].apply(readability_score)\n",
    "\n",
    "# Loading BERT model and tokenizer for embeddings (as before)\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0030e2f4",
   "metadata": {},
   "source": [
    "## Syntactic Complexity, Lexical Richness, Semantic Coherence, Sentiment Score, Burstiness and Perplexity features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "774d6b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def syntactic_complexity(text):\n",
    "    # Breaking the text into sentences and then tokenize each sentence into words.\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    # Counting the total number of sentences and tokens.\n",
    "    total_sentences = len(sentences)\n",
    "    total_tokens = sum(len(token) for token in tokens)\n",
    "    # Calculating the average length of a sentence as a measure of complexity.\n",
    "    avg_sentence_length = total_tokens / total_sentences if total_sentences > 0 else 0\n",
    "    return avg_sentence_length\n",
    "\n",
    "def lexical_richness(text):\n",
    "    # Tokenizing the text and find the set of unique words (types).\n",
    "    tokens = word_tokenize(text)\n",
    "    types = set(tokens)\n",
    "    # The Type-Token Ratio (TTR) is the number of unique words divided by the total number of words.\n",
    "    ttr = len(types) / len(tokens) if len(tokens) > 0 else 0\n",
    "    return ttr\n",
    "\n",
    "def semantic_coherence(text, num_topics=5):\n",
    "     # Lowercase and tokenize the text, preparing it for topic modeling\n",
    "    tokens = [word_tokenize(text.lower())]\n",
    "     # Return NaN if tokens are empty or missing\n",
    "    if not tokens or all(not token for token in tokens):\n",
    "        return np.nan    \n",
    "    dictionary = Dictionary(tokens)\n",
    "    if len(dictionary) == 0:  \n",
    "        return np.nan    \n",
    "    corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "    if not corpus:\n",
    "        return np.nan   \n",
    "    # Performing LDA to find topics and calculate the coherence score.\n",
    "    lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=42)\n",
    "    top_topics = lda.top_topics(corpus)\n",
    "    coherence_score = sum(score for topic, score in top_topics) / num_topics if top_topics else np.nan\n",
    "    return coherence_score\n",
    "\n",
    "def sentiment_score(text):\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    # Getting the compound sentiment score, which ranges from -1 (negative) to 1 (positive).\n",
    "    score = analyser.polarity_scores(text)\n",
    "    return score['compound']\n",
    "\n",
    "from nltk import FreqDist\n",
    "\n",
    "def calculate_burstiness(text):\n",
    "    # Tokenizing the text and calculate the frequency distribution of words\n",
    "    words = word_tokenize(text.lower())\n",
    "    freq_dist = FreqDist(words)\n",
    "    frequencies = list(freq_dist.values())\n",
    "    # Calculating mean and standard deviation of word frequencies\n",
    "    mean_freq = np.mean(frequencies)\n",
    "    std_dev_freq = np.std(frequencies)\n",
    "    # Burstiness is the standard deviation divided by the mean frequency\n",
    "    burstiness = std_dev_freq / mean_freq if mean_freq > 0 else 0\n",
    "    return burstiness\n",
    "\n",
    "\n",
    "# Setting up the tokenizer and model for GPT-2\n",
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model_gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "def calculate_perplexity(text):\n",
    "     # If the text is empty or whitespace, return NaN.\n",
    "    if not text.strip(): \n",
    "        return float('nan')  \n",
    "    # Tokenizing the input text, keeping within model's maximum length\n",
    "    tokenize_input = tokenizer_gpt2.encode(text, add_special_tokens=True, max_length=1024, truncation=True)\n",
    "    # If no tokens, return NaN\n",
    "    if len(tokenize_input) == 0:\n",
    "        return float('nan')\n",
    "    # Converting tokens to tensor and calculate the loss with the GPT-2 model\n",
    "    tensor_input = torch.tensor([tokenize_input]).to(model_gpt2.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_gpt2(tensor_input, labels=tensor_input)\n",
    "        loss = outputs.loss\n",
    "     # Perplexity is the exponentiation of the loss, measuring how well the model predicts the text\n",
    "    return np.exp(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dc8741",
   "metadata": {},
   "source": [
    "### Applying the above functions to our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a36d432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\PC\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:269: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    }
   ],
   "source": [
    "merged_df['syntactic_complexity'] = merged_df['text'].apply(syntactic_complexity)\n",
    "merged_df['lexical_richness'] = merged_df['text'].apply(lexical_richness)\n",
    "merged_df['readability_score'] = merged_df['text'].apply(readability_score)\n",
    "merged_df['sentiment_score'] = merged_df['text'].apply(sentiment_score)\n",
    "merged_df['burstiness'] = merged_df['text'].apply(calculate_burstiness)\n",
    "merged_df['semantic_coherence'] = merged_df['text'].apply(lambda x: semantic_coherence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb42d680",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['perplexity'] = merged_df['text'].apply(calculate_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a05475",
   "metadata": {},
   "source": [
    "## N-Gram Feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c42381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import bigrams, trigrams, word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99a4d97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngram_features(text, n=2):\n",
    "    # First, breaking the text down into individual words\n",
    "    tokens = word_tokenize(text)\n",
    "     # Then, create=ing n-grams from these tokens. An n-gram is a sequence of 'n' tokens\n",
    "    ngrams_list = list(ngrams(tokens, n))\n",
    "    # We're interested in the unique n-grams here, so converting the list to a set and counting them\n",
    "    return len(set(ngrams_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a107c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['unique_bigrams'] = merged_df['text'].apply(lambda x: generate_ngram_features(x, 2))\n",
    "merged_df['unique_trigrams'] = merged_df['text'].apply(lambda x: generate_ngram_features(x, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c85cb1ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>readability_score</th>\n",
       "      <th>syntactic_complexity</th>\n",
       "      <th>lexical_richness</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>burstiness</th>\n",
       "      <th>semantic_coherence</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>unique_bigrams</th>\n",
       "      <th>unique_trigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the aftermath of the Nakaba, the Palestinia...</td>\n",
       "      <td>1</td>\n",
       "      <td>70.23</td>\n",
       "      <td>21.294118</td>\n",
       "      <td>0.424033</td>\n",
       "      <td>-0.9650</td>\n",
       "      <td>2.032830</td>\n",
       "      <td>1.000089e-12</td>\n",
       "      <td>6.519134</td>\n",
       "      <td>720</td>\n",
       "      <td>722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Rafal crossing is a major source of humani...</td>\n",
       "      <td>1</td>\n",
       "      <td>51.07</td>\n",
       "      <td>23.681818</td>\n",
       "      <td>0.424184</td>\n",
       "      <td>-0.9948</td>\n",
       "      <td>1.394122</td>\n",
       "      <td>1.000089e-12</td>\n",
       "      <td>7.640199</td>\n",
       "      <td>514</td>\n",
       "      <td>519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hezbollah has also said that it has launched r...</td>\n",
       "      <td>1</td>\n",
       "      <td>58.62</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.1280</td>\n",
       "      <td>0.310497</td>\n",
       "      <td>1.000089e-12</td>\n",
       "      <td>11.168272</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A number of people were injured, including a w...</td>\n",
       "      <td>1</td>\n",
       "      <td>59.30</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>-0.3400</td>\n",
       "      <td>0.390209</td>\n",
       "      <td>1.000089e-12</td>\n",
       "      <td>17.722048</td>\n",
       "      <td>26</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nadeem Anjarwalla, the regiona of Nigerias cap...</td>\n",
       "      <td>1</td>\n",
       "      <td>41.36</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>-0.7579</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>1.000089e-12</td>\n",
       "      <td>68.086239</td>\n",
       "      <td>29</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2085</th>\n",
       "      <td>It is a cinematic masterpiece that delves into...</td>\n",
       "      <td>0</td>\n",
       "      <td>47.52</td>\n",
       "      <td>25.600000</td>\n",
       "      <td>0.703125</td>\n",
       "      <td>0.9697</td>\n",
       "      <td>1.102310</td>\n",
       "      <td>1.000089e-12</td>\n",
       "      <td>31.175757</td>\n",
       "      <td>126</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2086</th>\n",
       "      <td>It takes audiences on an unforgettable cinemat...</td>\n",
       "      <td>0</td>\n",
       "      <td>30.20</td>\n",
       "      <td>26.800000</td>\n",
       "      <td>0.731343</td>\n",
       "      <td>0.9080</td>\n",
       "      <td>0.889031</td>\n",
       "      <td>1.000089e-12</td>\n",
       "      <td>31.824208</td>\n",
       "      <td>131</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2087</th>\n",
       "      <td>It is a timeless tale of hope and resilience t...</td>\n",
       "      <td>0</td>\n",
       "      <td>39.87</td>\n",
       "      <td>25.400000</td>\n",
       "      <td>0.692913</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>1.056791</td>\n",
       "      <td>1.000089e-12</td>\n",
       "      <td>26.974663</td>\n",
       "      <td>123</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088</th>\n",
       "      <td>Andy Dufresne Tim Robbins is a banker convicte...</td>\n",
       "      <td>0</td>\n",
       "      <td>76.52</td>\n",
       "      <td>14.923077</td>\n",
       "      <td>0.618557</td>\n",
       "      <td>0.8931</td>\n",
       "      <td>1.058548</td>\n",
       "      <td>1.000089e-12</td>\n",
       "      <td>55.711385</td>\n",
       "      <td>179</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>The redeeming feature of this film is that it ...</td>\n",
       "      <td>0</td>\n",
       "      <td>80.01</td>\n",
       "      <td>24.230769</td>\n",
       "      <td>0.546032</td>\n",
       "      <td>0.9918</td>\n",
       "      <td>1.146172</td>\n",
       "      <td>1.000089e-12</td>\n",
       "      <td>40.172597</td>\n",
       "      <td>299</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2065 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label  \\\n",
       "0     In the aftermath of the Nakaba, the Palestinia...      1   \n",
       "1     The Rafal crossing is a major source of humani...      1   \n",
       "2     Hezbollah has also said that it has launched r...      1   \n",
       "3     A number of people were injured, including a w...      1   \n",
       "4     Nadeem Anjarwalla, the regiona of Nigerias cap...      1   \n",
       "...                                                 ...    ...   \n",
       "2085  It is a cinematic masterpiece that delves into...      0   \n",
       "2086  It takes audiences on an unforgettable cinemat...      0   \n",
       "2087  It is a timeless tale of hope and resilience t...      0   \n",
       "2088  Andy Dufresne Tim Robbins is a banker convicte...      0   \n",
       "2089  The redeeming feature of this film is that it ...      0   \n",
       "\n",
       "      readability_score  syntactic_complexity  lexical_richness  \\\n",
       "0                 70.23             21.294118          0.424033   \n",
       "1                 51.07             23.681818          0.424184   \n",
       "2                 58.62             23.000000          0.869565   \n",
       "3                 59.30             13.500000          0.925926   \n",
       "4                 41.36             15.000000          0.866667   \n",
       "...                 ...                   ...               ...   \n",
       "2085              47.52             25.600000          0.703125   \n",
       "2086              30.20             26.800000          0.731343   \n",
       "2087              39.87             25.400000          0.692913   \n",
       "2088              76.52             14.923077          0.618557   \n",
       "2089              80.01             24.230769          0.546032   \n",
       "\n",
       "      sentiment_score  burstiness  semantic_coherence  perplexity  \\\n",
       "0             -0.9650    2.032830        1.000089e-12    6.519134   \n",
       "1             -0.9948    1.394122        1.000089e-12    7.640199   \n",
       "2              0.1280    0.310497        1.000089e-12   11.168272   \n",
       "3             -0.3400    0.390209        1.000089e-12   17.722048   \n",
       "4             -0.7579    0.408248        1.000089e-12   68.086239   \n",
       "...               ...         ...                 ...         ...   \n",
       "2085           0.9697    1.102310        1.000089e-12   31.175757   \n",
       "2086           0.9080    0.889031        1.000089e-12   31.824208   \n",
       "2087           0.9565    1.056791        1.000089e-12   26.974663   \n",
       "2088           0.8931    1.058548        1.000089e-12   55.711385   \n",
       "2089           0.9918    1.146172        1.000089e-12   40.172597   \n",
       "\n",
       "      unique_bigrams  unique_trigrams  \n",
       "0                720              722  \n",
       "1                514              519  \n",
       "2                 22               21  \n",
       "3                 26               25  \n",
       "4                 29               28  \n",
       "...              ...              ...  \n",
       "2085             126              126  \n",
       "2086             131              132  \n",
       "2087             123              125  \n",
       "2088             179              189  \n",
       "2089             299              309  \n",
       "\n",
       "[2065 rows x 11 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23eb380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_pickle(\"Features.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52d572c",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "1. Lu, X. (2010) Automatic analysis of syntactic complexity in second language writing. International Journal of Corpus Linguistics, 15(4), pp. 474-496.\n",
    "2. McCarthy, P.M. and Jarvis, S. (2010) MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment. Behavior Research Methods, 42(2), pp. 381-392.\n",
    "3. Church, K.W. and Gale, W.A. (1995) Poisson mixtures. Natural Language Engineering, 1(2), pp. 163-190.\n",
    "4. Juola, P. (2006) Authorship attribution. Foundations and Trends in Information Retrieval, 1(3), pp. 233-334.\n",
    "5. Kincaid, J.P., Fishburne, R.P., Jr., Rogers, R.L., and Chissom, B.S. (1975) Derivation of new readability formulas (Automated Readability Index, Fog Count and Flesch Reading Ease Formula) for Navy enlisted personnel. Research Branch Report 8-75, Naval Technical Training, U. S. Naval Air Station, Memphis, TN.\n",
    "6. Stamatatos, E. (2009) A survey of modern authorship attribution methods. Journal of the American Society for Information Science and Technology, 60(3), pp. 538-556.\n",
    "7. Liu, B. (2012) Sentiment analysis and opinion mining. Synthesis Lectures on Human Language Technologies, 5(1), pp. 1-167."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
