{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a49f4007",
   "metadata": {},
   "source": [
    "## Web Scraping Approach\n",
    "\n",
    "A web scraping process aimed at collecting news articles from Al Jazeera's online platform is performed. 3 libraries were used to carry out our web scraping process.\n",
    "\n",
    "- `requests`: Retrieving web page content.(Python Software Foundation, n.d.)\n",
    "- `BeautifulSoup`: Parsing and navigating the HTML structure of these pages for data extraction.(Mitchell & Richardson, n.d.)\n",
    "- `pandas`: To structure the scraped information into a format ready for further analysis. (McKinney, n.d.)\n",
    "\n",
    "Using the requests and BeautifulSoup libraries (Python Software Foundation 2022; Crummy 2021), text material from Project Gutenberg ebooks is scraped from the web. Based on random ebook IDs, we develop a method to scrape text excerpts and ebook titles. For analysis, the data that was scraped is kept in a pandas DataFrame. Until the desired number of ebooks is attained, the procedure is repeated. After that, the DataFrame is stored for later use in a pickle file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "132cb827",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected eBooks: 1/150\n",
      "Collected eBooks: 2/150\n",
      "Collected eBooks: 3/150\n",
      "Collected eBooks: 4/150\n",
      "Collected eBooks: 5/150\n",
      "Collected eBooks: 6/150\n",
      "Collected eBooks: 7/150\n",
      "Collected eBooks: 8/150\n",
      "Collected eBooks: 9/150\n",
      "Collected eBooks: 10/150\n",
      "Collected eBooks: 11/150\n",
      "Collected eBooks: 12/150\n",
      "Collected eBooks: 13/150\n",
      "Collected eBooks: 14/150\n",
      "Collected eBooks: 15/150\n",
      "Collected eBooks: 16/150\n",
      "Collected eBooks: 17/150\n",
      "Collected eBooks: 18/150\n",
      "Collected eBooks: 19/150\n",
      "Collected eBooks: 20/150\n",
      "Collected eBooks: 21/150\n",
      "Collected eBooks: 22/150\n",
      "Collected eBooks: 23/150\n",
      "Collected eBooks: 24/150\n",
      "Collected eBooks: 25/150\n",
      "Collected eBooks: 26/150\n",
      "Collected eBooks: 27/150\n",
      "Collected eBooks: 28/150\n",
      "Collected eBooks: 29/150\n",
      "Collected eBooks: 30/150\n",
      "Collected eBooks: 31/150\n",
      "Collected eBooks: 32/150\n",
      "Collected eBooks: 33/150\n",
      "Collected eBooks: 34/150\n",
      "Collected eBooks: 35/150\n",
      "Collected eBooks: 36/150\n",
      "Collected eBooks: 37/150\n",
      "Collected eBooks: 38/150\n",
      "Collected eBooks: 39/150\n",
      "Collected eBooks: 40/150\n",
      "Collected eBooks: 41/150\n",
      "Collected eBooks: 42/150\n",
      "Collected eBooks: 43/150\n",
      "Collected eBooks: 44/150\n",
      "Collected eBooks: 45/150\n",
      "Collected eBooks: 46/150\n",
      "Collected eBooks: 47/150\n",
      "Collected eBooks: 48/150\n",
      "Collected eBooks: 49/150\n",
      "Collected eBooks: 50/150\n",
      "Collected eBooks: 51/150\n",
      "Collected eBooks: 52/150\n",
      "Collected eBooks: 53/150\n",
      "Collected eBooks: 54/150\n",
      "Collected eBooks: 55/150\n",
      "Collected eBooks: 56/150\n",
      "Collected eBooks: 57/150\n",
      "Collected eBooks: 58/150\n",
      "Collected eBooks: 59/150\n",
      "Collected eBooks: 60/150\n",
      "Collected eBooks: 61/150\n",
      "Collected eBooks: 62/150\n",
      "Collected eBooks: 63/150\n",
      "Collected eBooks: 64/150\n",
      "Collected eBooks: 65/150\n",
      "Collected eBooks: 66/150\n",
      "Collected eBooks: 67/150\n",
      "Collected eBooks: 68/150\n",
      "Collected eBooks: 69/150\n",
      "Collected eBooks: 70/150\n",
      "Collected eBooks: 71/150\n",
      "Collected eBooks: 72/150\n",
      "Collected eBooks: 73/150\n",
      "Collected eBooks: 74/150\n",
      "Collected eBooks: 75/150\n",
      "Collected eBooks: 76/150\n",
      "Collected eBooks: 77/150\n",
      "Collected eBooks: 78/150\n",
      "Collected eBooks: 79/150\n",
      "Collected eBooks: 80/150\n",
      "Collected eBooks: 81/150\n",
      "Collected eBooks: 82/150\n",
      "Collected eBooks: 83/150\n",
      "Collected eBooks: 84/150\n",
      "Collected eBooks: 85/150\n",
      "Collected eBooks: 86/150\n",
      "Collected eBooks: 87/150\n",
      "Collected eBooks: 88/150\n",
      "Collected eBooks: 89/150\n",
      "Collected eBooks: 90/150\n",
      "Collected eBooks: 91/150\n",
      "Collected eBooks: 92/150\n",
      "Collected eBooks: 93/150\n",
      "Collected eBooks: 94/150\n",
      "Collected eBooks: 95/150\n",
      "Collected eBooks: 96/150\n",
      "Collected eBooks: 97/150\n",
      "Collected eBooks: 98/150\n",
      "Collected eBooks: 99/150\n",
      "Collected eBooks: 100/150\n",
      "Collected eBooks: 101/150\n",
      "Collected eBooks: 102/150\n",
      "Collected eBooks: 103/150\n",
      "Collected eBooks: 104/150\n",
      "Collected eBooks: 105/150\n",
      "Collected eBooks: 106/150\n",
      "Collected eBooks: 107/150\n",
      "Collected eBooks: 108/150\n",
      "Collected eBooks: 109/150\n",
      "Collected eBooks: 110/150\n",
      "Collected eBooks: 111/150\n",
      "Collected eBooks: 112/150\n",
      "Collected eBooks: 113/150\n",
      "Collected eBooks: 114/150\n",
      "Collected eBooks: 115/150\n",
      "Collected eBooks: 116/150\n",
      "Collected eBooks: 117/150\n",
      "Collected eBooks: 118/150\n",
      "Collected eBooks: 119/150\n",
      "Collected eBooks: 120/150\n",
      "Collected eBooks: 121/150\n",
      "Collected eBooks: 122/150\n",
      "Collected eBooks: 123/150\n",
      "Collected eBooks: 124/150\n",
      "Collected eBooks: 125/150\n",
      "Collected eBooks: 126/150\n",
      "Collected eBooks: 127/150\n",
      "Collected eBooks: 128/150\n",
      "Collected eBooks: 129/150\n",
      "Collected eBooks: 130/150\n",
      "Collected eBooks: 131/150\n",
      "Collected eBooks: 132/150\n",
      "Collected eBooks: 133/150\n",
      "Collected eBooks: 134/150\n",
      "Collected eBooks: 135/150\n",
      "Collected eBooks: 136/150\n",
      "Collected eBooks: 137/150\n",
      "Collected eBooks: 138/150\n",
      "Collected eBooks: 139/150\n",
      "Collected eBooks: 140/150\n",
      "Collected eBooks: 141/150\n",
      "Collected eBooks: 142/150\n",
      "Collected eBooks: 143/150\n",
      "Collected eBooks: 144/150\n",
      "Collected eBooks: 145/150\n",
      "Collected eBooks: 146/150\n",
      "Collected eBooks: 147/150\n",
      "Collected eBooks: 148/150\n",
      "Collected eBooks: 149/150\n",
      "Collected eBooks: 150/150\n",
      "Total rows: 150\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def scrape_gutenberg_ebook(ebook_id):\n",
    "    url = f'https://www.gutenberg.org/files/{ebook_id}/{ebook_id}-h/{ebook_id}-h.htm'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return None  # If the fetch fails, return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    title_tag = soup.find('title')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else 'No Title Found'\n",
    "    \n",
    "    body = soup.find('body')\n",
    "    if body:\n",
    "        text = ' '.join(body.get_text().split()[:2000])  # Limit to first 2000 words\n",
    "    else:\n",
    "        text = ''\n",
    "    \n",
    "    return {\n",
    "        'url': url,\n",
    "        'title': title,\n",
    "        'text': text,\n",
    "        'label': 'Human-written'\n",
    "    }\n",
    "\n",
    "# Initializing an empty list to store the ebook data\n",
    "ebooks = []\n",
    "\n",
    "# Defining the target number of ebooks\n",
    "target_ebook_count = 150\n",
    "\n",
    "while len(ebooks) < target_ebook_count:\n",
    "    # Generating a random eBook ID \n",
    "    ebook_id = random.randint(1, 60000) \n",
    "    \n",
    "    result = scrape_gutenberg_ebook(ebook_id)\n",
    "    \n",
    "    # Checking if the eBook was successfully scraped and has content\n",
    "    if result and result['text']:\n",
    "        ebooks.append(result)\n",
    "        print(f\"Collected eBooks: {len(ebooks)}/{target_ebook_count}\")  # Progress output\n",
    "\n",
    "ebooks_df = pd.DataFrame(ebooks)\n",
    "\n",
    "print(f\"Total rows: {len(ebooks_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "333c0d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   url     150 non-null    object\n",
      " 1   title   150 non-null    object\n",
      " 2   text    150 non-null    object\n",
      " 3   label   150 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 4.8+ KB\n"
     ]
    }
   ],
   "source": [
    "ebooks_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f172cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ebooks_df['text'] == '').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8557f3a0",
   "metadata": {},
   "source": [
    "## Data storage for further analysis\n",
    "\n",
    "After successfully scraping and organizing the data, it is stored in a pickle file named `aljazeera_articles.pkl`. This step allowed us to keep a stable and easily accessible dataset for further analysis, obviating the need to redo the scraping process. Opting for a pickle file as the storage medium was particularly advantageous due to its capacity to store Python objects, thereby maintaining the integrity of the data's structure and content. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd49abdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebooks_df.to_pickle(\"gutenberg_data_300.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a856d31d",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- Python Software Foundation. (n.d.). *Requests: HTTP for Humansâ„¢*. Retrieved from [https://requests.readthedocs.io](https://requests.readthedocs.io)\n",
    "\n",
    "- Richard Mitchell, Leonard Richardson. (n.d.). *Beautiful Soup Documentation*. Retrieved from [https://www.crummy.com/software/BeautifulSoup/bs4/doc/](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "\n",
    "- Wes McKinney. (n.d.). *pandas: powerful Python data analysis toolkit*. Retrieved from [https://pandas.pydata.org/pandas-docs/stable/index.html](https://pandas.pydata.org/pandas-docs/stable/index.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
